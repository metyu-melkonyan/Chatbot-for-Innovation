{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jljjUiMspmlt"
      },
      "source": [
        "#Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QUW0m6v0pqVC",
        "outputId": "920ceed9-b46d-4c4f-8b7d-9f0060ca2206"
      },
      "outputs": [],
      "source": [
        "#!pip install --use-deprecated=legacy-resolver langchain openai chromadb pdfplumber beautifulsoup4 requests pyngrok langflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_xqgGELpxwJ"
      },
      "source": [
        "# Run Langflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsNJALAjy15R",
        "outputId": "59d9bf10-ccfe-4d72-e842-1d3bd93f7386"
      },
      "outputs": [],
      "source": [
        "#!ngrok authtoken 2usKOFYirN9IGNIcDQZ9qGsvqx6_6GkRrkmoVyBjTeHBKYg4C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-5fYOXqA8la"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"  # your actual OpenAI key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr3NcDNWpzMV",
        "outputId": "6b5f6e10-cfe0-4dcb-da77-15d2d6c2d182"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Langflow as a background process\n",
        "def run_langflow():\n",
        "    subprocess.run([\"langflow\", \"run\"])\n",
        "\n",
        "# Run Langflow in a separate thread\n",
        "thread = threading.Thread(target=run_langflow)\n",
        "thread.start()\n",
        "\n",
        "# Wait a few seconds for Langflow to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Expose Langflow using ngrok\n",
        "public_url = ngrok.connect(7860).public_url\n",
        "print(f\"Langflow is running at: {public_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLvI-krcpeK4"
      },
      "source": [
        "#Load our Data and Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iE-uUyPL731d",
        "outputId": "1f9a5919-4b4a-47c7-9236-8ca98883ed5c"
      },
      "outputs": [],
      "source": [
        "### Extract text from pdf\n",
        "\n",
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"/content/group13_courseproject.pdf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r603j0U48BPU",
        "outputId": "684bb620-155b-4f80-926d-e4662abafa61"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Combine all extracted content\n",
        "combined_text = pdf_text\n",
        "\n",
        "#pdf_text + \"\\n\\n\" + csv_text + \"\\n\\n\" + article_text\n",
        "\n",
        "# Split into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.create_documents([combined_text])\n",
        "\n",
        "# Create and store in Chroma DB\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "chroma_db.add_documents(docs)\n",
        "\n",
        "# Optional: persist to disk (useful across sessions)\n",
        "chroma_db.persist()\n",
        "\n",
        "print(\"Data stored in Chroma DB.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBF2up0CqbMX"
      },
      "source": [
        "#Create Chatbot Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KZeEmLVIqZXh",
        "outputId": "e4be5ffa-cba1-4ab6-b3d9-38e2ac164ebe"
      },
      "outputs": [],
      "source": [
        "# Imports using latest LangChain (v0.1.0+) structure\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# Load persisted Chroma DB\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever = chroma_db.as_retriever()\n",
        "\n",
        "#  Memory for multi-turn chat\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "#  Use ChatOpenAI (not OpenAI) for GPT-4, GPT-4o, GPT-3.5\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",  # Or \"gpt-3.5-turbo\" if you prefer\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "#  Define Conversational Retrieval Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"What are Canada's main innovation challenges?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNs--e-jBsfS",
        "outputId": "dc4c3436-928b-47f8-84b2-6017b749c54f"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"What are the primary reasons for Canada’s decline in the Global Innovation Index over the past decade?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nga4PNIm8_J7",
        "outputId": "3f460004-52a3-4dc9-85e9-05cbe613bc94"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"how can Canada improve in terms of innovations?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXgPTx5C8iBb"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"how does Canada measure among other countries in terms of innovations?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmt7BPunAuzt"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"How could Canada better compete with countries like China and the U.S. in AI research and development?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy7AVWyBAw7q"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"What story does Figure 4 tell about the imbalance between R&D and marketing in Canada?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjd9-cRHAxOV"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"How does Figure 9’s research output comparison reflect Canada’s academic strengths?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw94qpH-BMZp"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"What specific actions are recommended to reduce SR&ED processing times\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ct36dWlBd72"
      },
      "outputs": [],
      "source": [
        "# Ask a question using .invoke() instead of old call syntax\n",
        "query = \"Why does the report suggest placing SR&ED consultants directly in innovation hubs?\"\n",
        "response = qa_chain.invoke({\"question\": query})\n",
        "print(response[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
